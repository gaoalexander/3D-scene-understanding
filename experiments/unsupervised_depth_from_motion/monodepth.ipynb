{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import csv\n",
    "import datetime\n",
    "import random\n",
    "from path import Path\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import custom_transforms\n",
    "from utils import tensor2array, save_checkpoint\n",
    "from datasets.sequence_folders import SequenceFolder\n",
    "\n",
    "from loss_functions import compute_smooth_loss, compute_photo_and_geometry_loss, compute_errors\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import helper, data_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetEncoder(nn.Module):\n",
    "    \"\"\"Pytorch module for a resnet encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, pretrained, num_input_images=1):\n",
    "        super(ResnetEncoder, self).__init__()\n",
    "\n",
    "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
    "\n",
    "        resnets = {18: models.resnet18,\n",
    "                   34: models.resnet34,\n",
    "                   50: models.resnet50,\n",
    "                   101: models.resnet101,\n",
    "                   152: models.resnet152}\n",
    "\n",
    "        if num_layers not in resnets:\n",
    "            raise ValueError(\"{} is not a valid number of resnet layers\".format(num_layers))\n",
    "\n",
    "        if num_input_images > 1:\n",
    "            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n",
    "        else:\n",
    "            self.encoder = resnets[num_layers](pretrained)\n",
    "\n",
    "        if num_layers > 34:\n",
    "            self.num_ch_enc[1:] *= 4\n",
    "\n",
    "    def forward(self, input_image):\n",
    "        self.features = []\n",
    "        x = input_image\n",
    "        x = self.encoder.conv1(x)\n",
    "        x = self.encoder.bn1(x)\n",
    "        self.features.append(self.encoder.relu(x))\n",
    "        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n",
    "        self.features.append(self.encoder.layer2(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer3(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer4(self.features[-1]))\n",
    "\n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetMultiImageInput(models.ResNet):\n",
    "    \"\"\"Constructs a resnet model with varying number of input images.\n",
    "    Adapted from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "    \"\"\"\n",
    "    def __init__(self, block, layers, num_classes=1000, num_input_images=1):\n",
    "        super(ResNetMultiImageInput, self).__init__(block, layers)\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            num_input_images * 3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def resnet_multiimage_input(num_layers, pretrained=False, num_input_images=1):\n",
    "    \"\"\"Constructs a ResNet model.\n",
    "    Args:\n",
    "        num_layers (int): Number of resnet layers. Must be 18 or 50\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        num_input_images (int): Number of frames stacked as input\n",
    "    \"\"\"\n",
    "    assert num_layers in [18, 50], \"Can only run with 18 or 50 layer resnet\"\n",
    "    blocks = {18: [2, 2, 2, 2], 50: [3, 4, 6, 3]}[num_layers]\n",
    "    block_type = {18: models.resnet.BasicBlock, 50: models.resnet.Bottleneck}[num_layers]\n",
    "    model = ResNetMultiImageInput(block_type, blocks, num_input_images=num_input_images)\n",
    "\n",
    "    if pretrained:\n",
    "        loaded = model_zoo.load_url(models.resnet.model_urls['resnet{}'.format(num_layers)])\n",
    "        loaded['conv1.weight'] = torch.cat(\n",
    "            [loaded['conv1.weight']] * num_input_images, 1) / num_input_images\n",
    "        model.load_state_dict(loaded)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDecoder(nn.Module):\n",
    "    def __init__(self, num_ch_enc, num_input_features=1, num_frames_to_predict_for=1, stride=1):\n",
    "        super(PoseDecoder, self).__init__()\n",
    "\n",
    "        self.num_ch_enc = num_ch_enc\n",
    "        self.num_input_features = num_input_features\n",
    "\n",
    "        if num_frames_to_predict_for is None:\n",
    "            num_frames_to_predict_for = num_input_features - 1\n",
    "        self.num_frames_to_predict_for = num_frames_to_predict_for\n",
    "\n",
    "        self.convs = OrderedDict()\n",
    "        self.convs[(\"squeeze\")] = nn.Conv2d(self.num_ch_enc[-1], 256, 1)\n",
    "        self.convs[(\"pose\", 0)] = nn.Conv2d(num_input_features * 256, 256, 3, stride, 1)\n",
    "        self.convs[(\"pose\", 1)] = nn.Conv2d(256, 256, 3, stride, 1)\n",
    "        self.convs[(\"pose\", 2)] = nn.Conv2d(256, 6 * num_frames_to_predict_for, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.net = nn.ModuleList(list(self.convs.values()))\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        last_features = [f[-1] for f in input_features]\n",
    "\n",
    "        cat_features = [self.relu(self.convs[\"squeeze\"](f)) for f in last_features]\n",
    "        cat_features = torch.cat(cat_features, 1)\n",
    "\n",
    "        out = cat_features\n",
    "        for i in range(3):\n",
    "            out = self.convs[(\"pose\", i)](out)\n",
    "            if i != 2:\n",
    "                out = self.relu(out)\n",
    "\n",
    "        out = out.mean(3).mean(2)\n",
    "\n",
    "        pose = 0.01 * out.view(-1, 6)\n",
    "\n",
    "        return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers = 18, pretrained = True):\n",
    "        super(PoseResNet, self).__init__()\n",
    "        self.encoder = ResnetEncoder(num_layers = num_layers, pretrained = pretrained, num_input_images=2)\n",
    "        self.decoder = PoseDecoder(self.encoder.num_ch_enc)\n",
    "\n",
    "    def init_weights(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        x = torch.cat([img1,img2],1)\n",
    "        features = self.encoder(x)\n",
    "        pose = self.decoder([features])\n",
    "        return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = PoseResNet().to(device)\n",
    "model.train()\n",
    "\n",
    "tgt_img = torch.randn(4, 3, 256, 256).to(device)\n",
    "ref_imgs = [torch.randn(4, 3, 256, 256).to(device) for i in range(2)]\n",
    "\n",
    "pose = model(tgt_img, ref_imgs[0])\n",
    "\n",
    "print(pose.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Layer to perform a convolution followed by ELU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.conv = Conv3x3(in_channels, out_channels)\n",
    "        self.nonlin = nn.ELU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.nonlin(out)\n",
    "        return out\n",
    "\n",
    "class Conv3x3(nn.Module):\n",
    "    \"\"\"Layer to pad and convolve input\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_refl=True):\n",
    "        super(Conv3x3, self).__init__()\n",
    "\n",
    "        if use_refl:\n",
    "            self.pad = nn.ReflectionPad2d(1)\n",
    "        else:\n",
    "            self.pad = nn.ZeroPad2d(1)\n",
    "        self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pad(x)\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "def upsample(x):\n",
    "    \"\"\"Upsample input tensor by a factor of 2\n",
    "    \"\"\"\n",
    "    return F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "class DepthDecoder(nn.Module):\n",
    "    def __init__(self, num_ch_enc, scales=range(4), num_output_channels=1, use_skips=True):\n",
    "        super(DepthDecoder, self).__init__()\n",
    "\n",
    "        self.alpha = 10\n",
    "        self.beta = 0.01\n",
    "\n",
    "        self.num_output_channels = num_output_channels\n",
    "        self.use_skips = use_skips\n",
    "        self.upsample_mode = 'nearest'\n",
    "        self.scales = scales\n",
    "\n",
    "        self.num_ch_enc = num_ch_enc\n",
    "        self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n",
    "\n",
    "        # decoder\n",
    "        self.convs = OrderedDict()\n",
    "        for i in range(4, -1, -1):\n",
    "            # upconv_0\n",
    "            num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]\n",
    "            num_ch_out = self.num_ch_dec[i]\n",
    "            self.convs[(\"upconv\", i, 0)] = ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "            # upconv_1\n",
    "            num_ch_in = self.num_ch_dec[i]\n",
    "            if self.use_skips and i > 0:\n",
    "                num_ch_in += self.num_ch_enc[i - 1]\n",
    "            num_ch_out = self.num_ch_dec[i]\n",
    "            self.convs[(\"upconv\", i, 1)] = ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        for s in self.scales:\n",
    "            self.convs[(\"dispconv\", s)] = Conv3x3(self.num_ch_dec[s], self.num_output_channels)\n",
    "\n",
    "        self.decoder = nn.ModuleList(list(self.convs.values()))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        self.outputs = []\n",
    "\n",
    "        # decoder\n",
    "        x = input_features[-1]\n",
    "        for i in range(4, -1, -1):\n",
    "            x = self.convs[(\"upconv\", i, 0)](x)\n",
    "            x = [upsample(x)]\n",
    "            if self.use_skips and i > 0:\n",
    "                x += [input_features[i - 1]]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.convs[(\"upconv\", i, 1)](x)\n",
    "            if i in self.scales:\n",
    "                self.outputs.append(self.alpha * self.sigmoid(self.convs[(\"dispconv\", i)](x)) + self.beta)\n",
    "\n",
    "        self.outputs = self.outputs[::-1]\n",
    "        return self.outputs\n",
    "\n",
    "\n",
    "class DispResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers = 18, pretrained = True):\n",
    "        super(DispResNet, self).__init__()\n",
    "        self.encoder = ResnetEncoder(num_layers = num_layers, pretrained = pretrained, num_input_images=1)\n",
    "        self.decoder = DepthDecoder(self.encoder.num_ch_enc)\n",
    "\n",
    "    def init_weights(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        outputs = self.decoder(features)\n",
    "        \n",
    "        if self.training:\n",
    "            return outputs\n",
    "        else:\n",
    "            return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "model = DispResNet().to(device)\n",
    "model.train()\n",
    "\n",
    "B = 12\n",
    "\n",
    "tgt_img = torch.randn(B, 3, 256, 256).to(device)\n",
    "ref_imgs = [torch.randn(B, 3, 256, 256).to(device) for i in range(2)]\n",
    "\n",
    "tgt_depth = model(tgt_img)\n",
    "\n",
    "print(tgt_depth[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/Users/alexandergao/Documents/DeepLearning/Competition/competition/data'\n",
    "n_epochs = 200\n",
    "batch_size = 1\n",
    "\n",
    "lr = 0.0001\n",
    "beta = 0.999\n",
    "momentum = 0.9\n",
    "best_error = -1\n",
    "smooth_loss_weight = 0.1\n",
    "photo_loss_weight = 1\n",
    "geometry_consistency_weight = 0.5\n",
    "iters = 0\n",
    "\n",
    "resnet_layers = 50 #can also be set to 50\n",
    "n_scales = 1\n",
    "sequence_length = 3\n",
    "ssim = True\n",
    "mask = True\n",
    "auto_mask = True\n",
    "pretrain = True\n",
    "padding_mode = 'zeros'\n",
    "\n",
    "stats_update_rate = 10\n",
    "\n",
    "# parser.add_argument('--weight-decay', '--wd', default=0, type=float, metavar='W', help='weight decay')\n",
    "# parser.add_argument('--print-freq', default=10, type=int, metavar='N', help='print frequency')\n",
    "# parser.add_argument('--log-summary', default='progress_log_summary.csv', metavar='PATH', help='csv where to save per-epoch train and valid stats')\n",
    "# parser.add_argument('--log-full', default='progress_log_full.csv', metavar='PATH', help='csv where to save per-gradient descent train stats')\n",
    "# parser.add_argument('--log-output', action='store_true', help='will log dispnet outputs at validation step')\n",
    "# parser.add_argument('--num-scales', '--number-of-scales', type=int, help='the number of scales', metavar='W', default=1)\n",
    "# parser.add_argument('-p', '--photo-loss-weight', type=float, help='weight for photometric loss', metavar='W', default=1)\n",
    "# parser.add_argument('-s', '--smooth-loss-weight', type=float, help='weight for disparity smoothness loss', metavar='W', default=0.1)\n",
    "# parser.add_argument('-c', '--geometry-consistency-weight', type=float, help='weight for depth consistency loss', metavar='W', default=0.5)\n",
    "# parser.add_argument('--with-ssim', type=int, default=1, help='with ssim or not')\n",
    "# parser.add_argument('--with-mask', type=int, default=1, help='with the the mask for moving objects and occlusions or not')\n",
    "# parser.add_argument('--with-auto-mask', type=int,  default=0, help='with the the mask for stationary points')\n",
    "# parser.add_argument('--with-pretrain', type=int,  default=1, help='with or without imagenet pretrain for resnet')\n",
    "# parser.add_argument('--dataset', type=str, choices=['kitti', 'cs', 'nyu'], default='kitti', help='the dataset to train')\n",
    "# parser.add_argument('--pretrained-disp', dest='pretrained_disp', default=None, metavar='PATH', help='path to pre-trained dispnet model')\n",
    "# parser.add_argument('--pretrained-pose', dest='pretrained_pose', default=None, metavar='PATH', help='path to pre-trained Pose net model')\n",
    "# parser.add_argument('--name', dest='name', type=str, required=True, help='name of the experiment, checkpoints are stored in checpoints/name')\n",
    "# parser.add_argument('--padding-mode', type=str, choices=['zeros', 'border'], default='zeros',\n",
    "#                     help='padding mode for image warping : this is important for photometric differenciation when going outside target image.'\n",
    "#                          ' zeros will null gradients outside target image.'\n",
    "#                          ' border will only null gradients of the coordinate outside (x or y)')\n",
    "# parser.add_argument('--with-gt', action='store_true', help='use ground truth for validation. \\\n",
    "#                     You need to store it in npy 2D arrays see data/kitti_raw_loader.py for an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_loader, disp_net, pose_net, optimizer, train_writer):\n",
    "    global photo_loss_weight, smooth_loss_weight, geometry_consistency_weight\n",
    "    w1, w2, w3 = photo_loss_weight, smooth_loss_weight, geometry_consistency_weight\n",
    "\n",
    "    # switch to train mode\n",
    "    disp_net.train()\n",
    "    pose_net.train()\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    for i, (tgt_img, ref_imgs, intrinsics, intrinsics_inv) in enumerate(trainloader):\n",
    "\n",
    "        # measure data loading time\n",
    "        tgt_img = tgt_img.to(device)\n",
    "        ref_imgs = [img.to(device) for img in ref_imgs]\n",
    "        intrinsics = intrinsics.to(device)\n",
    "\n",
    "        # compute output\n",
    "        tgt_depth, ref_depths = compute_depth(disp_net, tgt_img, ref_imgs)\n",
    "        poses, poses_inv = compute_pose_with_inv(pose_net, tgt_img, ref_imgs)\n",
    "\n",
    "        loss_1, loss_3 = compute_photo_and_geometry_loss(tgt_img, ref_imgs, intrinsics, tgt_depth, ref_depths,\n",
    "                                                         poses, poses_inv, n_scales, ssim,\n",
    "                                                         mask, auto_mask, padding_mode)\n",
    "\n",
    "        loss_2 = compute_smooth_loss(tgt_depth, tgt_img, ref_depths, ref_imgs)\n",
    "\n",
    "        loss = w1*loss_1 + w2*loss_2 + w3*loss_3\n",
    "\n",
    "        if i % stats_update_rate == 0:\n",
    "            print('photometric_error', loss_1.item(), n_iter)\n",
    "            print('disparity_smoothness_loss', loss_2.item(), n_iter)\n",
    "            print('geometry_consistency_loss', loss_3.item(), n_iter)\n",
    "            print('total_loss', loss.item(), n_iter)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Train: Time {} Data {} Loss {}'.format(batch_time, data_time, losses))\n",
    "\n",
    "        iters += 1\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_without_gt(args, val_loader, disp_net, pose_net, epoch, output_writers=[]):\n",
    "    global device\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter(i=4, precision=4)\n",
    "    log_outputs = len(output_writers) > 0\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    disp_net.eval()\n",
    "    pose_net.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (tgt_img, ref_imgs, intrinsics, intrinsics_inv) in enumerate(val_loader):\n",
    "        tgt_img = tgt_img.to(device)\n",
    "        ref_imgs = [img.to(device) for img in ref_imgs]\n",
    "        intrinsics = intrinsics.to(device)\n",
    "        intrinsics_inv = intrinsics_inv.to(device)\n",
    "\n",
    "        # compute output\n",
    "        tgt_depth = [1 / disp_net(tgt_img)]\n",
    "        ref_depths = []\n",
    "        for ref_img in ref_imgs:\n",
    "            ref_depth = [1 / disp_net(ref_img)]\n",
    "            ref_depths.append(ref_depth)\n",
    "\n",
    "        if log_outputs and i < len(output_writers):\n",
    "            if epoch == 0:\n",
    "                output_writers[i].add_image('val Input', tensor2array(tgt_img[0]), 0)\n",
    "\n",
    "            output_writers[i].add_image('val Dispnet Output Normalized',\n",
    "                                        tensor2array(1/tgt_depth[0][0], max_value=None, colormap='magma'),\n",
    "                                        epoch)\n",
    "            output_writers[i].add_image('val Depth Output',\n",
    "                                        tensor2array(tgt_depth[0][0], max_value=10),\n",
    "                                        epoch)\n",
    "\n",
    "        poses, poses_inv = compute_pose_with_inv(pose_net, tgt_img, ref_imgs)\n",
    "\n",
    "        loss_1, loss_3 = compute_photo_and_geometry_loss(tgt_img, ref_imgs, intrinsics, tgt_depth, ref_depths,\n",
    "                                                         poses, poses_inv, n_scales, ssim,\n",
    "                                                         mask, False, padding_mode)\n",
    "\n",
    "        loss_2 = compute_smooth_loss(tgt_depth, tgt_img, ref_depths, ref_imgs)\n",
    "\n",
    "        loss_1 = loss_1.item()\n",
    "        loss_2 = loss_2.item()\n",
    "        loss_3 = loss_3.item()\n",
    "\n",
    "        loss = loss_1\n",
    "        losses.update([loss, loss_1, loss_2, loss_3])\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % args.print_freq == 0:\n",
    "            print('valid: Time {} Loss {}'.format(batch_time, losses))\n",
    "\n",
    "    return losses.avg, ['Total loss', 'Photo loss', 'Smooth loss', 'Consistency loss']\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_with_gt(args, val_loader, disp_net, epoch, output_writers=[]):\n",
    "    global device\n",
    "    batch_time = AverageMeter()\n",
    "    error_names = ['abs_diff', 'abs_rel', 'sq_rel', 'a1', 'a2', 'a3']\n",
    "    errors = AverageMeter(i=len(error_names))\n",
    "    log_outputs = len(output_writers) > 0\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    disp_net.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (tgt_img, depth) in enumerate(val_loader):\n",
    "        tgt_img = tgt_img.to(device)\n",
    "        depth = depth.to(device)\n",
    "\n",
    "        # check gt\n",
    "        if depth.nelement() == 0:\n",
    "            continue\n",
    "\n",
    "        # compute output\n",
    "        output_disp = disp_net(tgt_img)\n",
    "        output_depth = 1/output_disp[:, 0]\n",
    "\n",
    "        if log_outputs and i < len(output_writers):\n",
    "            if epoch == 0:\n",
    "                output_writers[i].add_image('val Input', tensor2array(tgt_img[0]), 0)\n",
    "                depth_to_show = depth[0]\n",
    "                output_writers[i].add_image('val target Depth',\n",
    "                                            tensor2array(depth_to_show, max_value=10),\n",
    "                                            epoch)\n",
    "                depth_to_show[depth_to_show == 0] = 1000\n",
    "                disp_to_show = (1/depth_to_show).clamp(0, 10)\n",
    "                output_writers[i].add_image('val target Disparity Normalized',\n",
    "                                            tensor2array(disp_to_show, max_value=None, colormap='magma'),\n",
    "                                            epoch)\n",
    "\n",
    "            output_writers[i].add_image('val Dispnet Output Normalized',\n",
    "                                        tensor2array(output_disp[0], max_value=None, colormap='magma'),\n",
    "                                        epoch)\n",
    "            output_writers[i].add_image('val Depth Output',\n",
    "                                        tensor2array(output_depth[0], max_value=10),\n",
    "                                        epoch)\n",
    "\n",
    "        if depth.nelement() != output_depth.nelement():\n",
    "            b, h, w = depth.size()\n",
    "            output_depth = torch.nn.functional.interpolate(output_depth.unsqueeze(1), [h, w]).squeeze(1)\n",
    "\n",
    "        errors.update(compute_errors(depth, output_depth, args.dataset))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % args.print_freq == 0:\n",
    "            print('valid: Time {} Abs Error {:.4f} ({:.4f})'.format(batch_time, errors.val[0], errors.avg[0]))\n",
    "    return errors.avg, error_names\n",
    "\n",
    "def compute_depth(disp_net, tgt_img, ref_imgs):\n",
    "    tgt_depth = [1/disp for disp in disp_net(tgt_img)]\n",
    "\n",
    "    ref_depths = []\n",
    "    for ref_img in ref_imgs:\n",
    "        ref_depth = [1/disp for disp in disp_net(ref_img)]\n",
    "        ref_depths.append(ref_depth)\n",
    "\n",
    "    return tgt_depth, ref_depths\n",
    "\n",
    "def compute_pose_with_inv(pose_net, tgt_img, ref_imgs):\n",
    "    poses = []\n",
    "    poses_inv = []\n",
    "    for ref_img in ref_imgs:\n",
    "        poses.append(pose_net(tgt_img, ref_img))\n",
    "        poses_inv.append(pose_net(ref_img, tgt_img))\n",
    "\n",
    "    return poses, poses_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> will save everything to training_checkpoints/save/05-01-20:47\n",
      "=> fetching scenes in '/Users/alexandergao/Documents/DeepLearning/Competition/competition/data'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: Path('/Users/alexandergao/Documents/DeepLearning/Competition/competition/data/train.txt')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-97ca0750dd10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                            \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                            \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                            sequence_length=sequence_length)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# if no Groundtruth is avalaible, Validation set is the same type as training set to measure photometric loss from warping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DeepLearning/Competition/git/auto-chauffeur/experiments/unsupervised_depth_from_motion/datasets/sequence_folders.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, seed, train, sequence_length, transform, target_transform)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mscene_list_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'train.txt'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'val.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_list_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl_folders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: Path('/Users/alexandergao/Documents/DeepLearning/Competition/competition/data/train.txt')"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%m-%d-%H:%M\")\n",
    "checkpoint_save_path = '/'.join(['training_checkpoints', save_path, timestamp])\n",
    "\n",
    "print('=> will save everything to {}'.format(checkpoint_save_path))\n",
    "os.makedirs(checkpoint_save_path, exist_ok=True)\n",
    "    \n",
    "output_writers = []\n",
    "\n",
    "# Data loading code\n",
    "normalize = custom_transforms.Normalize(mean=[0.45, 0.45, 0.45],\n",
    "                                        std=[0.225, 0.225, 0.225])\n",
    "\n",
    "train_transform = custom_transforms.Compose([custom_transforms.RandomHorizontalFlip(),\n",
    "                                             custom_transforms.RandomScaleCrop(),\n",
    "                                             custom_transforms.ArrayToTensor(),\n",
    "                                             normalize])\n",
    "\n",
    "valid_transform = custom_transforms.Compose([custom_transforms.ArrayToTensor(),\n",
    "                                             normalize])\n",
    "\n",
    "print(\"=> fetching scenes in '{}'\".format(datadir))\n",
    "train_set = SequenceFolder(datadir,\n",
    "                           transform=train_transform,\n",
    "                           seed=random_seed,\n",
    "                           train=True,\n",
    "                           sequence_length=sequence_length)\n",
    "\n",
    "# if no Groundtruth is avalaible, Validation set is the same type as training set to measure photometric loss from warping\n",
    "# if args.with_gt:\n",
    "#     from datasets.validation_folders import ValidationSet\n",
    "#     val_set = ValidationSet(\n",
    "#         args.data,\n",
    "#         transform=valid_transform\n",
    "#     )\n",
    "# else:\n",
    "val_set = SequenceFolder(datadir,\n",
    "                         transform=valid_transform,\n",
    "                         seed=random_seed,\n",
    "                         train=False,\n",
    "                         sequence_length=sequence_length)\n",
    "\n",
    "print('{} samples found in {} train scenes'.format(len(train_set), len(train_set.scenes)))\n",
    "print('{} samples found in {} valid scenes'.format(len(val_set), len(val_set.scenes)))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_set,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_set,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=2,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "disp_net = DispResNet(resnet_layers, pretrain).to(device)\n",
    "pose_net = PoseResNet(18, pretrain).to(device)\n",
    "\n",
    "# load parameters\n",
    "if resume_training:\n",
    "    print(\"=> using pre-trained weights for DispResNet\")\n",
    "    weights = torch.load(pretrained_disp)\n",
    "    disp_net.load_state_dict(weights['state_dict'], strict=False)\n",
    "\n",
    "    print(\"=> using pre-trained weights for PoseResNet\")\n",
    "    weights = torch.load(pretrained_pose)\n",
    "    pose_net.load_state_dict(weights['state_dict'], strict=False)\n",
    "\n",
    "disp_net = torch.nn.DataParallel(disp_net)\n",
    "pose_net = torch.nn.DataParallel(pose_net)\n",
    "\n",
    "optim_params = [{'params': disp_net.parameters(), 'lr': lr},\n",
    "                {'params': pose_net.parameters(), 'lr': lr}]\n",
    "\n",
    "optim = torch.optim.Adam(optim_params, betas=(momentum, beta), weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Avg Loss : 3.003\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-b1c6e516ff69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisp_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpose_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' * Avg Loss : {:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "########## TRAINING LOOP #########\n",
    "##################################\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # train for one epoch\n",
    "    train_loss = train(args, train_loader, disp_net, pose_net, optimizer, training_writer)\n",
    "    print(' * Avg Loss : {:.3f}'.format(train_loss))\n",
    "\n",
    "    # evaluate on validation set\n",
    "    if args.with_gt:\n",
    "        errors, error_names = validate_with_gt(args, val_loader, disp_net, epoch, output_writers)\n",
    "    else:\n",
    "        errors, error_names = validate_without_gt(args, val_loader, disp_net, pose_net, epoch, output_writers)\n",
    "    error_string = ', '.join('{} : {:.3f}'.format(name, error) for name, error in zip(error_names, errors))\n",
    "    print(' * Avg {}'.format(error_string))\n",
    "\n",
    "    for error, name in zip(errors, error_names):\n",
    "        training_writer.add_scalar(name, error, epoch)\n",
    "\n",
    "    # Up to you to chose the most relevant error to measure your model's performance, careful some measures are to maximize (such as a1,a2,a3)\n",
    "    decisive_error = errors[1]\n",
    "    if best_error < 0:\n",
    "        best_error = decisive_error\n",
    "\n",
    "    # remember lowest error and save checkpoint\n",
    "    is_best = decisive_error < best_error\n",
    "    best_error = min(best_error, decisive_error)\n",
    "    save_checkpoint(\n",
    "        checkpoint_save_path, {\n",
    "            'epoch': epoch + 1,\n",
    "            'disp_net_state_dict': disp_net.module.state_dict(),\n",
    "            'pose_net_state_dict': pose_net.module.state_dict(),\n",
    "        },\n",
    "        is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
