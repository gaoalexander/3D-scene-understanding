{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "cudnn.fastest = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_dim, out_dim):\n",
    "    return nn.Sequential(nn.Conv2d(in_dim, in_dim, kernel_size=3, stride=1, padding=1),\n",
    "                         nn.ELU(True),\n",
    "                         nn.Conv2d(in_dim, in_dim, kernel_size=3, stride=1, padding=1),\n",
    "                         nn.ELU(True),\n",
    "                         nn.Conv2d(in_dim, out_dim, kernel_size=1, stride=1, padding=0),\n",
    "                         nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "def deconv_block(in_dim, out_dim):\n",
    "    return nn.Sequential(nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
    "                         nn.ELU(True),\n",
    "                         nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
    "                         nn.ELU(True),\n",
    "                         nn.UpsamplingNearest2d(scale_factor=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_ch, ndf, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # 256\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(n_ch, ndf, kernel_size=3, stride=1, padding=1),\n",
    "                                   nn.ELU(True))\n",
    "        # 256\n",
    "        self.conv2 = conv_block(ndf, ndf)\n",
    "        # 128\n",
    "        self.conv3 = conv_block(ndf, ndf*2)\n",
    "        # 64\n",
    "        self.conv4 = conv_block(ndf*2, ndf*3)\n",
    "        # 32\n",
    "        self.conv5 = conv_block(ndf*3, ndf*4)\n",
    "        # 16\n",
    "        #self.conv6 = conv_block(ndf*4, ndf*4)\n",
    "        # 8\n",
    "        self.encode = nn.Conv2d(ndf*4, hidden_size, kernel_size=8, stride=1, padding=0)\n",
    "        # 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) \n",
    "        x = self.conv2(x) \n",
    "        x = self.conv3(x) \n",
    "        x = self.conv4(x) \n",
    "        x = self.conv5(x) \n",
    "        #x = self.conv6(x) \n",
    "        x = self.encode(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    #Used as both decoder in Discriminator, as well as Generator.  They do NOT share weights however.\n",
    "    def __init__(self, n_ch, ngf, hidden_size, condition=False, condition_size=0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.condition = condition\n",
    "\n",
    "        self.decode_cond = nn.ConvTranspose2d(condition_size, ngf, kernel_size=8,stride=1,padding=0)\n",
    "        # 1\n",
    "        self.decode = nn.ConvTranspose2d(hidden_size, ngf, kernel_size=8,stride=1,padding=0)\n",
    "        # 8\n",
    "        self.dconv6 = deconv_block(ngf*2, ngf)\n",
    "        # 16\n",
    "        self.dconv5 = deconv_block(ngf, ngf)\n",
    "        # 32\n",
    "        self.dconv4 = deconv_block(ngf, ngf)\n",
    "        # 64 \n",
    "        self.dconv3 = deconv_block(ngf, ngf)\n",
    "        # 128 \n",
    "        #self.dconv2 = deconv_block(ngf, ngf)\n",
    "        # 256\n",
    "        self.dconv1 = nn.Sequential(nn.Conv2d(ngf,ngf,kernel_size=3,stride=1,padding=1),\n",
    "                                    nn.ELU(True),\n",
    "                                    nn.Conv2d(ngf,ngf,kernel_size=3,stride=1,padding=1),\n",
    "                                    nn.ELU(True),\n",
    "                                    nn.Conv2d(ngf, n_ch,kernel_size=3, stride=1,padding=1),\n",
    "                                    nn.Tanh())\n",
    "\n",
    "    def forward(self, x, condition_vec=None):\n",
    "        x = self.decode(x)\n",
    "        if self.condition:\n",
    "            # NOTE: embedding condition vector\n",
    "            x_cond = self.decode_cond(condition_vec)\n",
    "            # NOTE: concatenation of z and condition vector\n",
    "            x = torch.cat([x, x_cond], 1)\n",
    "        x = self.dconv6(x) \n",
    "        x = self.dconv5(x) \n",
    "        x = self.dconv4(x) \n",
    "        x = self.dconv3(x) \n",
    "        #x = self.dconv2(x) \n",
    "        x = self.dconv1(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_ch, ndf, ngf, hidden_size, condition=False, condition_size=0):\n",
    "        super(D, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(n_ch, ndf, hidden_size)\n",
    "        self.decoder = Decoder(n_ch, ngf, hidden_size, condition, condition_size)\n",
    "\n",
    "    def forward(self, x, condition_vec=None):\n",
    "        h = self.encoder(x)\n",
    "        # NOTE injecting condition vector in Decoder\n",
    "        out = self.decoder(h, condition_vec)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  100\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', default='folder',  help='dataset name (It does not need to be modified)')\n",
    "# parser.add_argument('--dataroot', default='', help='path to trn dataset')\n",
    "# parser.add_argument('--batchSize', type=int, default=16, help='input batch size')\n",
    "# parser.add_argument('--valBatchSize', type=int, default=64, help='input batch size')\n",
    "# parser.add_argument('--originalSize', type=int, default=142, help='the height / width of the original input image')\n",
    "# parser.add_argument('--imageSize', type=int, default=128, help='the height / width of the cropped input image to network')\n",
    "# parser.add_argument('--inputChannelSize', type=int, default=3, help='size of the input channels')\n",
    "# parser.add_argument('--outputChannelSize', type=int, default=3, help='size of the output channels')\n",
    "# parser.add_argument('--ngf', type=int, default=128)\n",
    "# parser.add_argument('--ndf', type=int, default=128)\n",
    "# parser.add_argument('--hidden_size', type=int, default=64, help='bottleneck dimension of Discriminator')\n",
    "# parser.add_argument('--cond_size', type=int, default=2,  help='Whether to use conditional GAN')\n",
    "# parser.add_argument('--niter', type=int, default=30, help='number of epochs to train for')\n",
    "# parser.add_argument('--lrD', type=float, default=0.00005, help='learning rate')\n",
    "# parser.add_argument('--lrG', type=float, default=0.00005, help='learning rate')\n",
    "# parser.add_argument('--annealStart', type=int, default=0, help='annealing learning rate start to')\n",
    "# parser.add_argument('--annealEvery', type=int, default=30, help='epoch to reaching at learning rate of 0')\n",
    "# parser.add_argument('--lambda_k', type=float, default=0.001, help='learning rate of k')\n",
    "# parser.add_argument('--gamma', type=float, default=0.7, help='balance bewteen D and G')\n",
    "# parser.add_argument('--wd', type=float, default=0.0000, help='weight decay in D')\n",
    "# parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam')\n",
    "# parser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\n",
    "# parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
    "# parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "# parser.add_argument('--exp', default='sample', help='folder to output images and model checkpoints')\n",
    "# parser.add_argument('--display', type=int, default=5, help='interval for displaying train-logs')\n",
    "# parser.add_argument('--evalIter', type=int, default=4000, help='interval for evauating(generating) images from valDataroot')\n",
    "# opt = parser.parse_args()\n",
    "# print(opt)\n",
    "\n",
    "# create_exp_dir(opt.exp)\n",
    "\n",
    "manual_seed = 100\n",
    "#opt.manualSeed = random.randint(1, 10000)\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "print(\"Random Seed: \", manual_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-b9c45f0332e3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-b9c45f0332e3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    dataset =\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dataset = \n",
    "dataroot = \n",
    "workers =\n",
    "\n",
    "batch_size = 16\n",
    "original_size = 142\n",
    "image_size = 128\n",
    "ngf = \n",
    "ndf = \n",
    "\n",
    "in_ch_size =\n",
    "out_ch_size = \n",
    "cond_size = \n",
    "\n",
    "n_epochs = 100\n",
    "lr_g = 0.00005\n",
    "lr_d = 0.00005\n",
    "beta1 = \n",
    "wd =  #weight decay\n",
    "\n",
    "output_dir = \n",
    "\n",
    "g_weights_path = \n",
    "d_weights_path = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE get dataloader\n",
    "dataloader = getLoader(dataset, \n",
    "                       dataroot, \n",
    "                       originalS_size, \n",
    "                       image_size, \n",
    "                       batchS_size, \n",
    "                       workers,\n",
    "                       mean=(0.5, 0.5, 0.5),\n",
    "                       std=(0.5, 0.5, 0.5), \n",
    "                       split='train',\n",
    "                       shuffle=True, \n",
    "                       seed=manual_seed)\n",
    "\n",
    "trainLogger = open('%s/train.log' % output_dir, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_net = Decoder(in_ch_size, ngf, hidden_size, True, cond_size)\n",
    "d_net = Discriminator(in_ch_size, ndf, ndf, hidden_size, True, cond_size)\n",
    "\n",
    "g_net.apply(weights_init)\n",
    "d_net.apply(weights_init)\n",
    "\n",
    "if g_weights_path: g_net.load_state_dict(torch.load(gen_weights_path))\n",
    "if d_weights_path: d_net.load_state_dict(torch.load(d_weights_path))\n",
    "\n",
    "g_net.train()\n",
    "d_net.train()\n",
    "\n",
    "g_net.cuda()\n",
    "d_net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_g = optim.Adam(g_net.parameters(), lr = opt.lr_g, betas = (beta1, 0.999), weight_decay=0.0)\n",
    "optim_d = optim.Adam(d_net.parameters(), lr = opt.lr_d, betas = (beta1, 0.999), weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_d = torch.FloatTensor(batch_size, in_ch_size, image_size, image_size)\n",
    "input_g = torch.FloatTensor(batch_size, hidden_size, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(opt.valBatchSize, hidden_size, 1, 1).uniform_(-1, 1)\n",
    "\n",
    "input_d = input_d.cuda()\n",
    "input_g = input_g.cuda()\n",
    "fixed_noise = fixed_noise.cuda()\n",
    "\n",
    "input_d = Variable(input_d)\n",
    "input_g = Variable(input_g)\n",
    "fixed_noise = Variable(fixed_noise, volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE training loop\n",
    "iters = 0\n",
    "k = 0 # control how much emphasis is put on L(G(z_D)) during gradient descent.\n",
    "M_global = AverageMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        input_cpu, condition = data\n",
    "        batch_size = input_cpu.size(0)\n",
    "\n",
    "        input_cpu = input_cpu.cuda(async=True)\n",
    "        input_d.data.resize_as_(input_cpu).copy_(input_cpu)\n",
    "        \n",
    "        # NOTE generate condition vector whose value is 1 or -1 (one-hot)\n",
    "        cond = Variable(torch.FloatTensor(input_cpu.size(0), cond_size, 1, 1).fill_(-1).cuda(async=True))\n",
    "        for idx in range(batch_size):\n",
    "            if condition[idx] == 0: \n",
    "                cond.data[idx,0,0,0] = 1\n",
    "                #cond.data[idx,1,0,0] = -1\n",
    "            else:\n",
    "                cond.data[idx,1,0,0] = 1\n",
    "                #cond.data[idx,0,0,0] = -1\n",
    "\n",
    "        #######################\n",
    "        # Train Discriminator #\n",
    "        #######################\n",
    "        for p in d_net.parameters(): p.requires_grad = True \n",
    "        d_net.zero_grad()\n",
    "\n",
    "        input_g.data.resize_(batch_size, hidden_size, 1, 1).uniform_(-1, 1)\n",
    "        recon_real = d_net(input_d, cond)\n",
    "        gen = g_net(input_g, cond)\n",
    "        gen = gen.detach()\n",
    "        recon_gen = d_net(gen, cond)\n",
    "\n",
    "        err_d_real = torch.mean(torch.abs(recon_real - input_d))\n",
    "        err_d_gen = torch.mean(torch.abs(recon_gen - gen))\n",
    "        errD = errD_real - k * errD_gen\n",
    "        errD.backward()\n",
    "        optim_d.step()\n",
    "\n",
    "        #######################\n",
    "        # Train Generator #\n",
    "        #######################\n",
    "        for p in d_net.parameters(): p.requires_grad = False\n",
    "        g_net.zero_grad()\n",
    "\n",
    "        # NOTE compute L_G\n",
    "        input_g.data.resize_(batch_size, opt.hidden_size, 1, 1).uniform_(-1, 1)\n",
    "        gen = g_net(input_g, cond)\n",
    "        recon_gen = d_net(gen, cond)\n",
    "        errG = torch.mean(torch.abs(recon_gen - gen))\n",
    "        errG.backward()\n",
    "        optim_g.step()\n",
    "        iters += 1\n",
    "\n",
    "        # NOTE compute k_t and M_global\n",
    "        balance = (opt.gamma * errD_real - errD_gen).data[0]\n",
    "        k = min(max(k + opt.lambda_k * balance, 0), 1)\n",
    "        measure = errD_real.data[0] + np.abs(balance)\n",
    "        M_global.update(measure, input_d.size(0))\n",
    "\n",
    "        # logging\n",
    "        if iters % opt.display == 0:\n",
    "            print('[%d/%d][%d/%d] Ld: %f Lg: %f, M_global: %f(%f), K: %f, balance.: %f lr: %f'\n",
    "                  % (epoch, n_epochs, i, len(dataloader),\n",
    "                     errD.data[0], errG.data[0],\n",
    "                     measure, M_global.avg, k, balance,\n",
    "                     optimizerG.param_groups[0]['lr']))\n",
    "            sys.stdout.flush()\n",
    "            trainLogger.write('%d\\t%f\\t%f\\t%f\\t%f\\t%f\\t%f\\n' % \\\n",
    "                              (i, errD.data[0], errG.data[0], measure, M_global.avg, k, balance))\n",
    "            trainLogger.flush()\n",
    "        if iters % opt.evalIter == 0:\n",
    "            cond = Variable(\n",
    "                torch.FloatTensor(opt.valBatchSize, cond_size, 1, 1).fill_(-1).cuda(async=True), \n",
    "                volatile=True)\n",
    "            for idx in range(opt.valBatchSize):\n",
    "                if np.random.uniform(0,1) > 0.5:\n",
    "                    cond.data[idx,0,0,0] = 1\n",
    "                    #cond.data[idx,1,0,0] = -1\n",
    "                else:\n",
    "                    cond.data[idx,1,0,0] = 1\n",
    "                    #cond.data[idx,0,0,0] = -1\n",
    "            gen = g_net(fixed_noise, cond)\n",
    "            recon_gen = d_net(gen, cond)\n",
    "            vutils.save_image(gen.data, '%s/epoch_%08d_iter%08d_gen.png' % \\\n",
    "                              (opt.exp, epoch, iters), nrow=8, normalize=True)\n",
    "            vutils.save_image(recon_gen.data, '%s/epoch_%08d_iter%08d_gen_recon.png' % \\\n",
    "                              (opt.exp, epoch, iters), nrow=8, normalize=True)\n",
    "            vutils.save_image(input_d.data, '%s/epoch_%08d_iter%08d_real.png' % \\\n",
    "                              (opt.exp, epoch, iters), nrow=8, normalize=True)\n",
    "            vutils.save_image(recon_real.data, '%s/epoch_%08d_iter%08d_real_recon.png' % \\\n",
    "                              (opt.exp, epoch, iters), nrow=8, normalize=True)\n",
    "\n",
    "  # learning rate annealing\n",
    "  if epoch >= opt.annealStart:\n",
    "    adjust_learning_rate(optimizerD, opt.lrD, epoch, opt.annealEvery)\n",
    "    adjust_learning_rate(optimizerG, opt.lrG, epoch, opt.annealEvery)\n",
    "\n",
    "  # do checkpointing\n",
    "  torch.save(g_net.state_dict(), '%s/G_epoch_%d.pth' % (opt.exp, epoch))\n",
    "  torch.save(d_net.state_dict(), '%s/D_epoch_%d.pth' % (opt.exp, epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
