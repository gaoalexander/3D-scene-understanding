{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import HTML\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/alexander/DL/data'\n",
    "annotation_csv = '/home/alexander/DL/data/annotation.csv'\n",
    "\n",
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "# unlabeled_trainset = UnlabeledDataset(image_folder=image_folder, scene_index=labeled_scene_index, first_dim='sample', transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(unlabeled_trainset, batch_size=3, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_trainset = LabeledDataset(image_folder=data_dir,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index,\n",
    "#                                   img_transform = transforms.ToTensor(),\n",
    "#                                   map_transform = None,\n",
    "                                  img_transform=transforms.Compose([transforms.CenterCrop(256),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "                                                                   ]),\n",
    "                                  map_transform=transforms.Compose([transforms.ToPILImage(),\n",
    "                                                                    transforms.Resize(256),\n",
    "                                                                    transforms.ToTensor()\n",
    "                                                                   ]),\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 256, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_trainset[1][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 18, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for images, road_maps in trainloader:\n",
    "    print(images.size())\n",
    "    break\n",
    "\n",
    "# # sample, target, road_image, extra = iter(trainloader).next()\n",
    "# # print(len(sample[0]))\n",
    "# # # print(torch.stack(sample).shape)\n",
    "# # print(road_image[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(road_image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The 6 images orgenized in the following order:\n",
    "# # CAM_FRONT_LEFT, CAM_FRONT, CAM_FRONT_RIGHT, CAM_BACK_LEFT, CAM_BACK, CAM_BACK_RIGHT\n",
    "# plt.imshow(torchvision.utils.make_grid(sample[4], nrow=3).numpy().transpose(1, 2, 0))\n",
    "# plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The road map layout is encoded into a binary array of size [800, 800] per sample \n",
    "# # Each pixel is 0.1 meter in physiscal space, so 800 * 800 is 80m * 80m centered at the ego car\n",
    "# # The ego car is located in the center of the map (400, 400) and it is always facing the left\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.imshow(road_image[4][0], cmap='binary');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a Resnet block\n",
    "    A resnet block is a conv block with skip connections\n",
    "    Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dim (int)           -- the number of channels in the conv layer.\n",
    "            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "            use_bias (bool)     -- if the conv layer uses bias or not\n",
    "        \"\"\"\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.resnet_block = nn.Sequential(nn.Conv2d(dim, dim, kernel_size=3, padding=1),\n",
    "                                          nn.BatchNorm2d(dim),\n",
    "                                          nn.ReLU(True),\n",
    "                                          nn.Dropout(0.5),\n",
    "                                          nn.Conv2d(dim, dim, kernel_size=3, padding=1),\n",
    "                                          nn.BatchNorm2d(dim)\n",
    "                                         )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.resnet_block(x)  # add skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    RESNET-based generator that consists of Resnet blocks + downsampling/upsampling operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, ngf, n_blocks=6, init_gain=0.02):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            in_ch (int)         -- the number of channels in input images\n",
    "            out_ch (int)        -- the number of channels in output images\n",
    "            ngf (int)           -- the number of filters in the last conv layer\n",
    "            n_blocks (int)      -- the number of ResNet blocks\n",
    "            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n",
    "        \"\"\"        \n",
    "        assert(n_blocks >= 0)\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            \n",
    "            nn.Conv2d(in_ch, ngf, kernel_size=7, padding=0),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Conv2d(ngf, ngf * 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Conv2d(ngf * 2, ngf * 4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            ResnetBlock(ngf * 4),  #1\n",
    "            ResnetBlock(ngf * 4),  #2\n",
    "            ResnetBlock(ngf * 4),  #3\n",
    "            ResnetBlock(ngf * 4),  #4\n",
    "            ResnetBlock(ngf * 4),  #5\n",
    "            ResnetBlock(ngf * 4),  #6\n",
    "            ResnetBlock(ngf * 4),  #7\n",
    "            ResnetBlock(ngf * 4),  #8\n",
    "            ResnetBlock(ngf * 4),  #9\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 4, int(ngf * 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(int(ngf * 2)),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 2, int(ngf), kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(int(ngf)),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(ngf, out_ch, kernel_size=7, padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_ch, ndf=64, n_layers=3):  \n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(nn.Conv2d(in_ch, ndf, kernel_size=4, stride=2, padding=1),\n",
    "                                    nn.LeakyReLU(0.2, True)\n",
    "                                   )\n",
    "        \n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        sequence2 = []        \n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence2 += [nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=4, stride=2, padding=1),\n",
    "                         nn.BatchNorm2d(ndf * nf_mult),\n",
    "                         nn.LeakyReLU(0.2, True)\n",
    "                         ]\n",
    "        self.block2 = nn.Sequential(*sequence2)\n",
    "        \n",
    "        sequence3 = []\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence3 += [nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=4, stride=1, padding=1),\n",
    "                     nn.BatchNorm2d(ndf * nf_mult),\n",
    "                     nn.LeakyReLU(0.2, True)\n",
    "                     ]\n",
    "        self.block3 = nn.Sequential(*sequence3)\n",
    "        \n",
    "        self.model = nn.Sequential(self.block1,\n",
    "                                   self.block2,\n",
    "                                   self.block3,\n",
    "                                   nn.Conv2d(ndf * nf_mult, 1, kernel_size=4, stride=1, padding=1)\n",
    "                                  )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ch = 18\n",
    "out_ch = 1\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "n_blocks_g = 9\n",
    "n_layers_d = 3\n",
    "\n",
    "n_epochs = 100\n",
    "lr_g = 0.0001\n",
    "lr_d = 0.0004\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "L1_lambda = 100\n",
    "\n",
    "real_label = 0.0\n",
    "gen_label = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to generator:\t torch.Size([1, 18, 256, 256])\n",
      "\n",
      "Output of generator:\t torch.Size([1, 1, 256, 256])\n",
      "\n",
      "Input to discriminator:\t torch.Size([1, 19, 256, 256])\n",
      "\n",
      "Output of discriminator: torch.Size([1, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check - dimensions of inputs and outputs to both networks.\n",
    "\n",
    "test = torch.zeros([1, 18, 256, 256])\n",
    "print(\"Input to generator:\\t\", test.size())\n",
    "\n",
    "test_generator = Generator(in_ch, out_ch, ngf, n_blocks=n_blocks_g, init_gain=0.02)\n",
    "test_result_g = test_generator(test)\n",
    "print(\"\\nOutput of generator:\\t\", test_result_g.size())\n",
    "\n",
    "test_d_input = torch.cat((test_result_g, test), 1)\n",
    "print(\"\\nInput to discriminator:\\t\", test_d_input.size())\n",
    "\n",
    "test_discriminator = Discriminator(in_ch + out_ch, ndf=ndf, n_layers=n_layers_d)\n",
    "test_result_d = test_discriminator(test_d_input)\n",
    "print(\"\\nOutput of discriminator:\", test_result_d.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The GANLoss class abstracts away the need to create the target label tensor\n",
    "    that has the same size as the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, gan_mode, real_label=1.0, gen_label=0.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n",
    "            target_real_label (bool) - - label for a real image\n",
    "            target_gen_label (bool) - - label of a generated image\n",
    "        Note: Do not use sigmoid as the last layer of Discriminator.\n",
    "        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n",
    "        \"\"\"\n",
    "        super(GANLoss, self).__init__()\n",
    "        \n",
    "        self.gan_mode = gan_mode\n",
    "        self.real_label = real_label\n",
    "        self.gen_label = gen_label\n",
    "        \n",
    "        if gan_mode == 'LS':\n",
    "            self.loss = nn.MSELoss().to(device)\n",
    "        elif gan_mode == 'BCE':\n",
    "            self.loss = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "    def get_target_tensor(self, output, target_is_real):\n",
    "        # Create label tensors with same size as the discriminator output.\n",
    "        if target_is_real:\n",
    "            if \"cuda\" in device:\n",
    "                target_tensor = torch.cuda.FloatTensor([self.real_label])\n",
    "            else:\n",
    "                target_tensor = torch.Tensor([self.real_label])\n",
    "        else:\n",
    "            if \"cuda\" in device:\n",
    "                target_tensor = torch.cuda.FloatTensor([self.gen_label])\n",
    "            else:\n",
    "                target_tensor = torch.Tensor([self.gen_label])\n",
    "                \n",
    "        return target_tensor.expand_as(output)\n",
    "\n",
    "    def __call__(self, output, target_is_real):\n",
    "        target_tensor = self.get_target_tensor(output, target_is_real)\n",
    "        loss = self.loss(output, target_tensor)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):  # define the initialization function\n",
    "    classname = m.__class__.__name__\n",
    "    if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "        init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(19, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(19, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (3): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator(in_ch, out_ch, ngf, n_blocks=n_blocks_g, init_gain=0.02).to(device)\n",
    "discriminator = Discriminator(in_ch + out_ch, ndf=ndf, n_layers=n_layers_d).to(device)\n",
    "\n",
    "generator.apply(init_weights)\n",
    "discriminator.apply(init_weights)\n",
    "\n",
    "generator.train()\n",
    "discriminator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We combine GAN Loss and L1 Loss to attain Total Loss\n",
    "criterion_gan = GANLoss('LS', real_label=real_label, gen_label=gen_label).to(device)\n",
    "criterion_L1 = nn.L1Loss().to(device)\n",
    "\n",
    "optim_G = torch.optim.Adam(generator.parameters(), lr=lr_g, betas=(beta1, beta2))\n",
    "optim_D = torch.optim.Adam(discriminator.parameters(), lr=lr_d, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "update_stats_rate = 50\n",
    "losses_g = []\n",
    "losses_d = []\n",
    "intermediate_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/441 [00:01<12:03,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tBatch: [1/441]\n",
      "Loss_D: 106.9700\tLoss_G: 13.4467\n",
      "D(real): -0.2425  ||  D(gen) Pre-G-step: -0.2395  ||  D(gen) Post-G-step: 1.5887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 51/441 [00:22<03:15,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tBatch: [51/441]\n",
      "Loss_D: 50.8093\tLoss_G: 25.3003\n",
      "D(real): 4.9717  ||  D(gen) Pre-G-step: 4.9662  ||  D(gen) Post-G-step: 4.9951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 101/441 [00:43<02:47,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tBatch: [101/441]\n",
      "Loss_D: 50.3071\tLoss_G: 25.0344\n",
      "D(real): 4.9711  ||  D(gen) Pre-G-step: 4.9632  ||  D(gen) Post-G-step: 4.9903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 151/441 [01:04<02:23,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tBatch: [151/441]\n",
      "Loss_D: 50.7675\tLoss_G: 25.3617\n",
      "D(real): 5.0421  ||  D(gen) Pre-G-step: 5.0358  ||  D(gen) Post-G-step: 5.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 201/441 [01:25<01:59,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tBatch: [201/441]\n",
      "Loss_D: 50.1238\tLoss_G: 24.6042\n",
      "D(real): 4.9999  ||  D(gen) Pre-G-step: 4.9985  ||  D(gen) Post-G-step: 4.9549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 250/441 [01:46<01:21,  2.35it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alexander/.conda/envs/DL/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/alexander/.conda/envs/DL/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/alexander/.conda/envs/DL/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/alexander/.conda/envs/DL/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-57eccae00433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m                   % (epoch + 1, n_epochs,\n\u001b[1;32m     64\u001b[0m                      \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                      \u001b[0mloss_d_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                      d_real_mean, d_gen_mean_pre, d_gen_mean_post))\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################################\n",
    "########## TRAINING LOOP ###########\n",
    "####################################\n",
    "for epoch in range(n_epochs):\n",
    "    i = 1\n",
    "    for real_18ch, real_map in tqdm(trainloader):\n",
    "        \"\"\"\n",
    "        TRAIN DISCRIMINATOR ON REAL AND GENERATED SAMPLES\n",
    "        \"\"\"\n",
    "        real_18ch = real_18ch.to(device)\n",
    "        real_map = real_map.to(device)\n",
    "        \n",
    "        gen_map = generator(real_18ch)\n",
    "        \n",
    "        real_19ch = torch.cat((real_18ch, real_map), 1).to(device)\n",
    "        gen_19ch = torch.cat((real_18ch, gen_map), 1).to(device)\n",
    "\n",
    "        output_real = discriminator(real_19ch)\n",
    "        output_gen = discriminator(gen_19ch.detach())\n",
    "        \n",
    "        d_real_mean = output_real.mean().item()\n",
    "        d_gen_mean_pre = output_gen.mean().item()\n",
    "\n",
    "        # Make label tensors.\n",
    "        target_real = torch.Tensor([real_label]).expand_as(output_real).to(device)\n",
    "        target_gen = torch.Tensor([gen_label]).expand_as(output_gen).to(device)\n",
    "\n",
    "        # Compute loss.\n",
    "#         loss_d_real = criterion_gan(output_real, target_is_real=True) + L1_lambda * criterion_L1(real_map, real_map)\n",
    "#         loss_d_gen = criterion_gan(output_gen, target_is_real=False) + L1_lambda * criterion_L1(gen_map, real_map)\n",
    "#         loss_d_total = (loss_d_real + loss_d_gen)\n",
    "\n",
    "        loss_d_real = criterion_gan(output_real, target_is_real=True)\n",
    "        loss_d_gen = criterion_gan(output_gen, target_is_real=False)\n",
    "        loss_d_total = (loss_d_real + loss_d_gen)  \n",
    "    \n",
    "        discriminator.zero_grad()\n",
    "        loss_d_total.backward()\n",
    "        optim_D.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        TRAIN GENERATOR\n",
    "        \"\"\"\n",
    "        output_gen = discriminator(gen_19ch)\n",
    "        target_real = torch.Tensor([real_label]).expand_as(output_real).to(device)\n",
    "#         loss_g = criterion_gan(output_gen, target_is_real=True) + L1_lambda * criterion_L1(gen_map, real_map)\n",
    "        loss_g = criterion_gan(output_gen, target_is_real=True)\n",
    "        d_gen_mean_post = output_gen.mean().item()\n",
    "        \n",
    "        generator.zero_grad()\n",
    "        loss_g.backward()\n",
    "        optim_G.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        COMPILE TRAINING STATISTICS\n",
    "        \"\"\"\n",
    "        losses_d.append(loss_d_total)\n",
    "        losses_g.append(loss_g)\n",
    "        \n",
    "        # Print out training stats\n",
    "        if i % update_stats_rate == 1:\n",
    "            print('Epoch: [%d/%d]\\tBatch: [%d/%d]\\nLoss_D: %.4f\\tLoss_G: %.4f\\nD(real): %.4f  ||  D(gen) Pre-G-step: %.4f  ||  D(gen) Post-G-step: %.4f'\n",
    "                  % (epoch + 1, n_epochs,\n",
    "                     i, len(trainloader),\n",
    "                     loss_d_total.item(), loss_g.item(),\n",
    "                     d_real_mean, d_gen_mean_pre, d_gen_mean_post))\n",
    "        \n",
    "        # Append current generated image to list of intermediate images to view post training\n",
    "        if (iters % update_stats_rate == 0) or epoch == n_epochs-1:\n",
    "            with torch.no_grad():\n",
    "                output_gen = generator(real_18ch).detach().cpu()\n",
    "            intermediate_images.append(vutils.make_grid(output_gen, padding=2, normalize=True))\n",
    "        \n",
    "        i += 1\n",
    "        iters += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(losses_g,label=\"G\")\n",
    "plt.plot(losses_d,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "fig = plt.figure()\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in intermediate_images]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=500, repeat_delay=500, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(trainloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
